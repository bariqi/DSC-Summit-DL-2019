{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"main.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"D6I8RXnshBoc","colab_type":"text"},"source":["## teks yang dimiringkan## Program People Counting"]},{"cell_type":"markdown","metadata":{"id":"aC38Db5Uj9VU","colab_type":"text"},"source":["### Arahkan colaboratory pada folder project"]},{"cell_type":"code","metadata":{"id":"CT3kJGCkeEN4","colab_type":"code","outputId":"74644d32-0605-4993-8d2d-ae307810cb8e","executionInfo":{"status":"ok","timestamp":1576565855650,"user_tz":-420,"elapsed":24202,"user":{"displayName":"BARIQI ABDILLAH 1206237536","photoUrl":"","userId":"16483724593283972369"}},"colab":{"base_uri":"https://localhost:8080/","height":128}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Yolowf08liy5","colab_type":"code","outputId":"b7bb8e7c-e037-4769-9003-6bed3a81bd62","executionInfo":{"status":"ok","timestamp":1576573608023,"user_tz":-420,"elapsed":7043,"user":{"displayName":"BARIQI ABDILLAH 1206237536","photoUrl":"","userId":"16483724593283972369"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["# %cd gdrive/My\\ Drive/object-counting\n","!ls"],"execution_count":13,"outputs":[{"output_type":"stream","text":["main.ipynb  MobileNetSSD_deploy.caffemodel  modul\t    Tol_TMII.mp4\n","main.py     MobileNetSSD_deploy.prototxt    Pedestrian.mp4\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pK9Soq4WkQp5","colab_type":"text"},"source":["### Import libraries yang dibutuhkan"]},{"cell_type":"code","metadata":{"id":"3GPgbNfw1lfB","colab_type":"code","colab":{}},"source":["from modul.centroidtracker import CentroidTracker\n","from modul.trackableobject import TrackableObject\n","from imutils.video import VideoStream\n","from imutils.video import FPS\n","import numpy as np\n","import imutils\n","import time\n","import dlib\n","import cv2"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TFRDKGqYkaix","colab_type":"text"},"source":["### Define variable"]},{"cell_type":"code","metadata":{"id":"4MqWNIRb2jSg","colab_type":"code","outputId":"37b0d444-a981-49fb-b830-4810bc337f88","executionInfo":{"status":"ok","timestamp":1576573616801,"user_tz":-420,"elapsed":1583,"user":{"displayName":"BARIQI ABDILLAH 1206237536","photoUrl":"","userId":"16483724593283972369"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["confidence_value = 0.4\n","skip_frames = 30\n","\n","prototxt = ('MobileNetSSD_deploy.prototxt')\n","model = ('MobileNetSSD_deploy.caffemodel')\n","video_output = 'output.avi'\n","\n","\n","# initialize the list of class labels MobileNet SSD was trained to  detect\n","CLASSES = [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\",\n","           \"sofa\", \"train\", \"tvmonitor\"]\n","\n","# load our serialized model from disk\n","print(\"[INFO] loading model...\")\n","net = cv2.dnn.readNetFromCaffe(prototxt, model)\n","\n","print(\"[INFO] opening video file...\")\n","vs = cv2.VideoCapture(\"Pedestrian.mp4\")\n","\n","# initialize none for vieowriter\n","writer = None\n","\n","# initialize the frame dimensions (we'll set them as soon as we read the first frame from the video)\n","W = None\n","H = None\n","\n","# instantiate our centroid tracker, then initialize a list to store each of our dlib correlation trackers, followed by a dictionary to map each unique object ID to a TrackableObject\n","ct = CentroidTracker(maxDisappeared=40, maxDistance=50)\n","trackers = []\n","trackableObjects = {}\n","\n","totalFrames = 0\n","totalDown = 0\n","totalUp = 0"],"execution_count":15,"outputs":[{"output_type":"stream","text":["[INFO] loading model...\n","[INFO] opening video file...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"s6MXCVjQHKB_","colab_type":"text"},"source":["### Counting main program"]},{"cell_type":"code","metadata":{"id":"dP-XVR2rTpnp","colab_type":"code","colab":{}},"source":["# loop over frames from the video stream\n","while (vs.isOpened()):\n","\t# VideoCapture or VideoStream\n","\tret,frame = vs.read()\n","\n","\tif not ret:\n","\t\tbreak;\n","\n","\tframe = imutils.resize(frame, width=500)\n","\trgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","\n","\t# if the frame dimensions are empty, set them\n","\tif W is None or H is None:\n","\t\t(H, W) = frame.shape[:2]\n","\n","\t# if we are supposed to be writing a video to disk, initialize the writer\n","\tif video_output is not None and writer is None:\n","\t\tfourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n","\t\twriter = cv2.VideoWriter(video_output, fourcc, 30, (W, H), True)\n","\n","\t# initialize the current status along with our list of bounding box rectangles returned by either (1) our object detector or (2) the correlation trackers\n","\trects = []\n","\n","\t# check to see if we should run a more computationally expensive object detection method to aid our tracker\n","\tif totalFrames % skip_frames == 0:\n","\t\t# set the status and initialize our new set of object trackers\n","\t\ttrackers = []\n","\n","\t\t# convert the frame to a blob and pass the blob through the network and obtain the detections\n","\t\tblob = cv2.dnn.blobFromImage(frame, 0.007843, (W, H), 127.5)\n","\t\tnet.setInput(blob)\n","\t\tdetections = net.forward()\n","\n","\t\t# loop over the detections\n","\t\tfor i in np.arange(0, detections.shape[2]):\n","\t\t\t# extract the confidence (i.e., probability) associated with the prediction\n","\t\t\tconfidence = detections[0, 0, i, 2]\n","\n","\t\t\t# filter out weak detections by requiring a minimum confidence\n","\t\t\tif confidence > confidence_value:\n","\n","\t\t\t\t# extract the index of the class prototxt from the detections list\n","\t\t\t\tidx = int(detections[0, 0, i, 1])\n","\n","\t\t\t\t# if the class prototxt is not a person, ignore it\n","\t\t\t\tif CLASSES[idx] != \"person\":\n","\t\t\t\t\tcontinue\n","\n","\t\t\t\t# compute the (x, y)-coordinates of the bounding box for the object\n","\t\t\t\tbox = detections[0, 0, i, 3:7] * np.array([W, H, W, H])\n","\t\t\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n","\n","\t\t\t\t# construct a dlib rectangle object from the bounding box coordinates and then start the dlib correlation tracker\n","\t\t\t\ttracker = dlib.correlation_tracker()\n","\t\t\t\trect = dlib.rectangle(startX, startY, endX, endY)\n","\t\t\t\ttracker.start_track(rgb, rect)\n","\n","\t\t\t\t# add the tracker to our list of trackers so we can utilize it during skip frames\n","\t\t\t\ttrackers.append(tracker)\n","\n","\t# otherwise, we should utilize our object *trackers* rather than object *detectors* to obtain a higher frame processing throughput\n","\telse:\n","\t\t# loop over the trackers\n","\t\tfor tracker in trackers:\n","\t\t\t# update the tracker and grab the updated position\n","\t\t\ttracker.update(rgb)\n","\t\t\tpos = tracker.get_position()\n","\n","\t\t\t# unpack the position object\n","\t\t\tstartX = int(pos.left())\n","\t\t\tstartY = int(pos.top())\n","\t\t\tendX = int(pos.right())\n","\t\t\tendY = int(pos.bottom())\n","\n","\t\t\t# add the bounding box coordinates to the rectangles list\n","\t\t\trects.append((startX, startY, endX, endY))\n","\n","\t# draw a horizontal line in the center of the frame -- once an object crosses this line we will determine whether they were moving 'up' or 'down'\n","\tcv2.line(frame, (0, H // 2), (W, H // 2), (255, 0, 0), 2)\n","\n","\t# use the centroid tracker to associate the (1) old object centroids with (2) the newly computed object centroids\n","\tobjects = ct.update(rects)\n","\n","\t# loop over the tracked objects\n","\tfor (objectID, centroid) in objects.items():\n","\t\t# check to see if a trackable object exists for the current object ID\n","\t\tto = trackableObjects.get(objectID, None)\n","\n","\t\t# if there is no existing trackable object, create one\n","\t\tif to is None:\n","\t\t\tto = TrackableObject(objectID, centroid)\n","\n","\t\t# otherwise, there is a trackable object so we can utilize it to determine direction\n","\t\telse:\n","\t\t\t# the difference between the y-coordinate of the *current*\n","\t\t\t# centroid and the mean of *previous* centroids will tell us in which direction the object is moving (negative for 'up' and positive for 'down')\n","\t\t\ty = [c[1] for c in to.centroids]\n","\t\t\tdirection = centroid[1] - np.mean(y)\n","\t\t\tto.centroids.append(centroid)\n","\n","\t\t\t# check to see if the object has been counted or not\n","\t\t\tif not to.counted:\n","\t\t\t\t# if the direction is negative (indicating the object is moving up) AND the centroid is above the center line, count the object\n","\t\t\t\tif direction < 0 and centroid[1] < H // 2:\n","\t\t\t\t\ttotalUp += 1\n","\t\t\t\t\tto.counted = True\n","\n","\t\t\t\t# if the direction is positive (indicating the object is moving down) AND the centroid is below the center line, count the object\n","\t\t\t\telif direction > 0 and centroid[1] > H // 2:\n","\t\t\t\t\ttotalDown += 1\n","\t\t\t\t\tto.counted = True\n","\n","\t\t# store the trackable object in our dictionary\n","\t\ttrackableObjects[objectID] = to\n","\n","\t\t# draw both the ID of the object and the centroid of the object on the output frame\n","\t\ttext = \"ID {}\".format(objectID)\n","\t\tcv2.putText(frame, text, (centroid[0] - 10, centroid[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n","\t\tcv2.circle(frame, (centroid[0], centroid[1]), 4, (0, 255, 0), -1)\n","\n","\t# construct a tuple of information we will be displaying on the frame\n","\tinfo = [\n","\t\t(\"Up\", totalUp),\n","\t\t(\"Down\", totalDown)\n","\t]\n","\n","\t# loop over the info tuples and draw them on our frame\n","\tfor (i, (k, v)) in enumerate(info):\n","\t\ttext = \"{}: {}\".format(k, v)\n","\t\tcv2.putText(frame, text, (10, H - ((i * 20) + 20)),\tcv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n","\n","\t# check to see if we should write the frame to disk\n","\tif writer is not None:\n","\t\twriter.write(frame)\n","\ttotalFrames += 1\n","\n","vs.release()\n","writer.release()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iP79Qr0yf201","colab_type":"code","colab":{}},"source":["\n"],"execution_count":0,"outputs":[]}]}